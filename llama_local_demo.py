import torch

from transformers import AutoTokenizer, AutoModelForCausalLM



# æ¨¡å‹è·¯å¾„ï¼ˆä»¥ Llama3.2-1B-Instruct ä¸ºä¾‹ï¼Œä½ ä¹Ÿå¯ä»¥è¯•è¯• 3B æˆ– 8Bï¼‰

model_path = "/share/apps/llama/Llama3.2-1B-Instruct"



# åŠ è½½ tokenizer å’Œæ¨¡å‹

tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map="auto")



# è¾“å…¥ prompt

prompt = "ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰"



inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(**inputs, max_new_tokens=100)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)



print("ğŸ¦™ LLaMA å›å¤ï¼š\n")

print(response)


